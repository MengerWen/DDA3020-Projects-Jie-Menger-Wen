{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Question 4: Convolutional Neural Network\n",
    "**Course Name:** Machine Learning (DDA3020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=Red>*Please enter your personal information (Double-click this block first)*</font>\n",
    "\n",
    "**Name:** 文杰\n",
    "\n",
    "**Student ID:** 123090612"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's highly recommended to finish Question 3 first.**\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this question, you will implement two CNN models and train them on the same dataset as Question 3 (Fasion-MNIST). We will discover how well-suited CNNs are for intensive data tasks such as image processing, compared to traditional machine learning algorithms (like those tree tree-based models in Question 3). Similarly, your task is to **run all codes in this script and complete the parts marked with** <font color=Red>\\[TASK\\]</font>.\n",
    "\n",
    "### Introduction of TensorFlow\n",
    "\n",
    "TensorFlow is a powerful open-source package for machine learning and deep learning, enabling efficient implementation of complex models like neural networks with ease. First of all, you need to install the TensorFlow package with the version of 2.9\n",
    "\n",
    "```bash\n",
    "pip install numpy==1.26 tensorflow==2.9 -i https://mirrors.aliyun.com/pypi/simple/ \n",
    "```\n",
    "\n",
    "by running this commend in your command line window. To check whether the package is successfully installed, you can try to run the following import block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need to carefully read this block since it's just loading the dataset. Just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind, subset=None):\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz'%kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz'%kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
    "    \n",
    "    if subset is not None:\n",
    "        selected_images, selected_labels = [], []\n",
    "        for label in range(10):\n",
    "            indices = np.where(labels == label)[0]\n",
    "            selected_indices = np.random.choice(indices, subset, replace=False)\n",
    "            selected_images.append(images[selected_indices])\n",
    "            selected_labels.append(labels[selected_indices])\n",
    "        images = np.concatenate(selected_images, axis=0)\n",
    "        labels = np.concatenate(selected_labels, axis=0)\n",
    "\n",
    "        paired = list(zip(images, labels))\n",
    "        random.shuffle(paired)\n",
    "        images, labels = zip(*paired)\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will use all data of Fashion-MNIST and do a little bit data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('./data/', kind='train')\n",
    "X_test, y_test = load_mnist('./data/', kind='t10k')\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "At the beginning, we need to build a very simple CNN model with the structure of\n",
    "1. A 2D convolutional layer with 16 filters with each size 3*3 (RELU activation function)\n",
    "2. A 2D maxpooling layer with 2*2 pooling window\n",
    "3. A flatten layer to convert 2D feature into 1D vector\n",
    "4. A fully connected layer using Softmax activation\n",
    "\n",
    "Remember that we are doing a image classification task, so we shall use categorical cross entropy function as the loss funtion. <font color=Red>\\[TASK\\]</font> (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MG\\Python\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7741 - loss: 0.6558 - val_accuracy: 0.8640 - val_loss: 0.3809\n",
      "Epoch 2/2\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8713 - loss: 0.3694 - val_accuracy: 0.8768 - val_loss: 0.3434\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - accuracy: 0.8775 - loss: 0.3562\n",
      "Simple CNN Test Accuracy: 0.8738999962806702\n"
     ]
    }
   ],
   "source": [
    "# Define the simple CNN model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add a convolutional layer with 16 filters, 3x3 kernel, and ReLU activation\n",
    "# Extracts basic features from the 28x28 grayscale images\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "# Add a max-pooling layer with 2x2 window to reduce spatial dimensions\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the 2D feature maps into a 1D vector for the dense layer\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Add a fully connected layer with 10 units and Softmax activation for classification\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model with Adam optimizer, categorical cross-entropy loss, and accuracy metric\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 2 epochs with a batch size of 32 and 10% validation split\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.1) # Train the model\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test) #  Test the model\n",
    "print(f\"Simple CNN Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Then we will take a challenge to implement a more complex CNN model to have a better classification performance. Here is a structure for your reference, or you can also design your own CNN model. The only requirement is to have a better performance than the simple CNN in Task 1 (a larger accuracy score on test set).\n",
    "\n",
    "The reference structure is devided into three parts:\n",
    "1. Primary Feature Extraction Part\n",
    "    1. A 2D convolutional layer with 32 filters with each size 3*3 (RELU activation function)\n",
    "    2. A normalization layer\n",
    "    3. A 2D maxpooling layer with 2*2 pooling window\n",
    "    4. A dropout layer (randomly drops 25% of units) (designed for preventing overfitting)\n",
    "2. Advanced Feature Extraction Part\n",
    "\n",
    "    This part is mostly similar to the previous section. The only difference is to use more filters (like 64) in convolutional layer to gain high-level features\n",
    "3. Classification Part\n",
    "    1. A flatten layer to convert 2D feature into 1D vector\n",
    "    2. A fully connected layer with 512 units using RELU to summerize high-dimensinal features\n",
    "    3. Another connected layer for Softmax classification\n",
    "\n",
    "Remember that we are doing a image classification task, so we shall use categorical cross entropy function as the loss funtion. <font color=Red>\\[TASK\\]</font> (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.7858 - loss: 0.6424 - val_accuracy: 0.8753 - val_loss: 0.3465\n",
      "Epoch 2/2\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.8826 - loss: 0.3158 - val_accuracy: 0.8978 - val_loss: 0.2818\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8959 - loss: 0.2892\n",
      "Complex CNN Test Accuracy: 0.8949999809265137\n"
     ]
    }
   ],
   "source": [
    "# Define the complex CNN model\n",
    "model_complex = models.Sequential()\n",
    "\n",
    "# Primary Feature Extraction Part\n",
    "# Convolutional layer with 32 filters, 3x3 kernel, and ReLU activation\n",
    "model_complex.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "# Batch normalization to stabilize and accelerate training\n",
    "model_complex.add(layers.BatchNormalization())\n",
    "\n",
    "# Max-pooling layer with 2x2 window\n",
    "model_complex.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Dropout layer to prevent overfitting by randomly dropping 25% of units\n",
    "model_complex.add(layers.Dropout(0.25))\n",
    "\n",
    "# Advanced Feature Extraction Part\n",
    "# Convolutional layer with 64 filters to capture higher-level features\n",
    "model_complex.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Another batch normalization layer\n",
    "model_complex.add(layers.BatchNormalization())\n",
    "\n",
    "# Another max-pooling layer\n",
    "model_complex.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Another dropout layer\n",
    "model_complex.add(layers.Dropout(0.25))\n",
    "\n",
    "# Classification Part\n",
    "# Flatten the feature maps into a 1D vector\n",
    "model_complex.add(layers.Flatten())\n",
    "\n",
    "# Fully connected layer with 512 units and ReLU activation to learn complex patterns\n",
    "model_complex.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "# Output layer with 10 units and Softmax activation for classification\n",
    "model_complex.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model with Adam optimizer, categorical cross-entropy loss, and accuracy metric\n",
    "model_complex.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 2 epochs with a batch size of 32 and 10% validation split\n",
    "model_complex.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_complex, test_acc_complex = model_complex.evaluate(X_test, y_test)\n",
    "print(f\"Complex CNN Test Accuracy: {test_acc_complex}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
